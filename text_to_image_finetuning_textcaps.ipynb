{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Stable Diffusion for Text Generation using TextCaps Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates fine-tuning Stable Diffusion v1.5 with LoRA adapters to improve text rendering capabilities using the TextCaps dataset.\n",
    "\n",
    "**Flow:**\n",
    "1. Image + text as input → Model\n",
    "2. Model trained with LoRA\n",
    "3. Give a prompt → Generate image\n",
    "4. Evaluate with OCR (Exact Match + Character Accuracy)\n",
    "\n",
    "**Dataset:** TextCaps (lmms-lab/TextCaps)\n",
    "**Model:** Stable Diffusion v1.5 with LoRA fine-tuning\n",
    "**Evaluation:** OCR-based readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SETUP: Installing Required Packages\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!pip install -q diffusers transformers accelerate peft datasets pillow pytesseract python-Levenshtein opencv-python-headless\n",
    "\n",
    "# Install Tesseract OCR for Colab\n",
    "!apt-get install -y tesseract-ocr\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"./text-render-lora-textcaps\"\n",
    "MAX_LENGTH = 77\n",
    "IMAGE_SIZE = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_STEPS = 800\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"  Device: {accelerator.device}\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Exploratory Data Analysis - TextCaps Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: EXPLORATORY DATA ANALYSIS - TextCaps Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load TextCaps dataset\n",
    "print(\"\\nLoading TextCaps dataset...\")\n",
    "dataset = load_dataset(\"lmms-lab/TextCaps\", split=\"train\", streaming=True)\n",
    "print(\"✓ TextCaps dataset loaded (streaming mode)\")\n",
    "\n",
    "# Sample initial examples for EDA\n",
    "print(\"\\nSampling examples for analysis...\")\n",
    "samples = []\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= 30:\n",
    "        break\n",
    "    samples.append(example)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} samples...\")\n",
    "\n",
    "print(f\"\\n✓ Sampled {len(samples)} examples\")\n",
    "\n",
    "# Analyze dataset structure\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Dataset Structure:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Available keys: {samples[0].keys()}\")\n",
    "\n",
    "# Analyze samples\n",
    "caption_lengths = []\n",
    "word_counts = []\n",
    "\n",
    "print(\"\\nSample Examples:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i in range(min(5, len(samples))):\n",
    "    sample = samples[i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    \n",
    "    # Get caption\n",
    "    caption = sample.get('caption', ['N/A'])[0] if isinstance(sample.get('caption'), list) else sample.get('caption', 'N/A')\n",
    "    \n",
    "    caption_lengths.append(len(caption))\n",
    "    word_counts.append(len(caption.split()))\n",
    "    \n",
    "    print(f\"  Caption: {caption[:100]}...\")\n",
    "    print(f\"  Length: {len(caption)} chars, {len(caption.split())} words\")\n",
    "    \n",
    "    # Image info\n",
    "    if 'image' in sample:\n",
    "        img = sample['image']\n",
    "        print(f\"  Image size: {img.size if hasattr(img, 'size') else 'N/A'}\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Dataset Statistics (from 30 samples):\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Avg caption length: {np.mean(caption_lengths):.1f} chars\")\n",
    "print(f\"  Avg word count: {np.mean(word_counts):.1f} words\")\n",
    "print(f\"  Min/Max length: {min(caption_lengths)}/{max(caption_lengths)} chars\")\n",
    "\n",
    "# Visualize samples\n",
    "print(\"\\nVisualizing sample images...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    sample = samples[i]\n",
    "    img = sample['image']\n",
    "    caption = sample.get('caption', ['N/A'])[0] if isinstance(sample.get('caption'), list) else sample.get('caption', 'N/A')\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{caption[:60]}...\", fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textcaps_samples_eda.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ EDA complete - samples saved to 'textcaps_samples_eda.png'\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Key Observations:\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "1. TextCaps contains images from COCO dataset with text descriptions\n",
    "2. Captions describe entire scenes, not just text content\n",
    "3. Text in images is incidental (signs, labels, products in context)\n",
    "4. Images are complex real-world scenes\n",
    "5. Not optimized for text rendering, but useful for text-in-context learning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Filtering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: DATA FILTERING AND PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\")\n",
    "\n",
    "def is_good_textcaps_sample(example):\n",
    "    \"\"\"Filter for simple text-focused images in TextCaps\"\"\"\n",
    "    caption = example.get('caption', [''])[0] if isinstance(example.get('caption'), list) else example.get('caption', '')\n",
    "    caption_lower = caption.lower()\n",
    "    \n",
    "    # Keywords indicating clear text presence\n",
    "    good_keywords = [\n",
    "        'says', 'reads', 'written', 'word', 'text', 'label', \n",
    "        'sign', 'logo', 'brand', 'name', 'titled', 'message',\n",
    "        'writing', 'letters', 'print', 'caption'\n",
    "    ]\n",
    "    \n",
    "    # Keywords indicating complex scenes\n",
    "    bad_keywords = [\n",
    "        'people', 'person', 'man', 'woman', 'child', 'crowd', 'group',\n",
    "        'wearing', 'holding', 'standing', 'sitting'\n",
    "    ]\n",
    "    \n",
    "    has_good = any(kw in caption_lower for kw in good_keywords)\n",
    "    has_bad = sum(kw in caption_lower for kw in bad_keywords)\n",
    "    is_short = len(caption.split()) < 25\n",
    "    \n",
    "    return has_good and has_bad <= 1 and is_short\n",
    "\n",
    "def preprocess_textcaps_sample(example):\n",
    "    \"\"\"Convert TextCaps sample to tensors\"\"\"\n",
    "    image = example['image'].convert('RGB').resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "    image_array = np.array(image).astype(np.float32) / 127.5 - 1.0\n",
    "    pixel_values = torch.from_numpy(image_array).permute(2, 0, 1)\n",
    "    \n",
    "    caption = example.get('caption', [''])[0] if isinstance(example.get('caption'), list) else example.get('caption', '')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        caption,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs.input_ids[0]\n",
    "    \n",
    "    return pixel_values, input_ids, caption\n",
    "\n",
    "# Filter and collect good samples\n",
    "print(\"\\nFiltering TextCaps dataset for text-focused samples...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "dataset_stream = load_dataset(\"lmms-lab/TextCaps\", split=\"train\", streaming=True)\n",
    "\n",
    "filtered_samples = []\n",
    "processed_count = 0\n",
    "\n",
    "for example in dataset_stream:\n",
    "    processed_count += 1\n",
    "    \n",
    "    if len(filtered_samples) >= 300:\n",
    "        break\n",
    "    \n",
    "    if processed_count >= 3000:\n",
    "        print(f\"  Searched {processed_count} samples, found {len(filtered_samples)}\")\n",
    "        break\n",
    "    \n",
    "    if is_good_textcaps_sample(example):\n",
    "        filtered_samples.append(example)\n",
    "        \n",
    "        if len(filtered_samples) % 50 == 0:\n",
    "            print(f\"  Found {len(filtered_samples)} suitable samples (searched {processed_count})...\")\n",
    "\n",
    "print(f\"\\n✓ Filtered dataset: {len(filtered_samples)} text-focused samples from {processed_count} total\")\n",
    "print(f\"  Filter ratio: {len(filtered_samples)/processed_count*100:.1f}%\")\n",
    "\n",
    "# Show filtered examples\n",
    "print(\"\\nFiltered Sample Captions:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(min(5, len(filtered_samples))):\n",
    "    caption = filtered_samples[i].get('caption', [''])[0] if isinstance(filtered_samples[i].get('caption'), list) else filtered_samples[i].get('caption', '')\n",
    "    print(f\"{i+1}. {caption}\")\n",
    "\n",
    "# Visualize filtered samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, len(filtered_samples))):\n",
    "    sample = filtered_samples[i]\n",
    "    img = sample['image']\n",
    "    caption = sample.get('caption', [''])[0] if isinstance(sample.get('caption'), list) else sample.get('caption', '')\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{caption[:60]}...\", fontsize=7)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textcaps_filtered_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Filtered samples visualization saved\")\n",
    "\n",
    "# Preprocess all samples\n",
    "print(\"\\nPreprocessing training data...\")\n",
    "preprocessed_data = []\n",
    "\n",
    "for sample in tqdm(filtered_samples, desc=\"Preprocessing\"):\n",
    "    try:\n",
    "        pixel_values, input_ids, caption = preprocess_textcaps_sample(sample)\n",
    "        preprocessed_data.append({\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': input_ids,\n",
    "            'caption': caption\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ {len(preprocessed_data)} samples preprocessed and ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: MODEL INITIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Core Components\n",
    "print(\"Loading Stable Diffusion v1.5 components...\")\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\")\n",
    "\n",
    "print(\"✓ Base models loaded\")\n",
    "\n",
    "# Freeze original weights\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"✓ Base model weights frozen\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "\n",
    "print(\"✓ LoRA adapters added to UNet\")\n",
    "\n",
    "# Move to device\n",
    "device = accelerator.device\n",
    "unet.to(device)\n",
    "vae.to(device, dtype=torch.float16)\n",
    "text_encoder.to(device, dtype=torch.float16)\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in unet.parameters())\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model Statistics:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"  Trainable percentage: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  VAE scaling factor: {vae.config.scaling_factor}\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STEP 4: TRAINING - {TRAIN_STEPS} steps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "unet.train()\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Training steps: {TRAIN_STEPS}\")\n",
    "print(f\"  Training samples: {len(preprocessed_data)}\")\n",
    "print(f\"  Epochs (approx): {TRAIN_STEPS / len(preprocessed_data):.1f}\")\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "unet_dtype = next(unet.parameters()).dtype\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "progress_bar = tqdm(range(TRAIN_STEPS), desc=\"Training\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    # Get data (cycle through dataset)\n",
    "    idx = step % len(preprocessed_data)\n",
    "    data_item = preprocessed_data[idx]\n",
    "    \n",
    "    pixel_values = data_item['pixel_values']\n",
    "    input_ids = data_item['input_ids']\n",
    "    \n",
    "    # Move to GPU and add batch dimension\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device, dtype=torch.float16)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    # VAE ENCODING - Convert image to latent space\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "    \n",
    "    # Create random noise\n",
    "    noise = torch.randn_like(latents)\n",
    "    \n",
    "    # Sample random timestep\n",
    "    timesteps = torch.randint(\n",
    "        0, \n",
    "        noise_scheduler.config.num_train_timesteps, \n",
    "        (1,), \n",
    "        device=device\n",
    "    ).long()\n",
    "    \n",
    "    # Add noise to latents (forward diffusion)\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    \n",
    "    # Get text embeddings\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "    \n",
    "    # Ensure dtype consistency\n",
    "    noisy_latents = noisy_latents.to(dtype=unet_dtype)\n",
    "    encoder_hidden_states = encoder_hidden_states.to(dtype=unet_dtype)\n",
    "    \n",
    "    # Predict noise using UNet\n",
    "    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        model_pred.float(), \n",
    "        noise.float(), \n",
    "        reduction=\"mean\"\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Record loss\n",
    "    loss_value = loss.item()\n",
    "    losses.append(loss_value)\n",
    "    \n",
    "    # Update progress bar\n",
    "    if step % 10 == 0:\n",
    "        avg_loss = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
    "        progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    # Detailed logging\n",
    "    if (step + 1) % 100 == 0:\n",
    "        avg_loss = np.mean(losses[-100:])\n",
    "        print(f\"\\n  Step {step+1}/{TRAIN_STEPS}: Avg Loss (last 100) = {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving model...\")\n",
    "unet.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"✓ Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'TextCaps',\n",
    "    'training_steps': TRAIN_STEPS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_samples': len(preprocessed_data),\n",
    "    'final_loss': losses[-1] if losses else None,\n",
    "    'avg_loss_last_100': np.mean(losses[-100:]) if len(losses) >= 100 else np.mean(losses)\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✓ Training metadata saved\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, alpha=0.3, label='Raw Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Raw)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 50\n",
    "if len(losses) >= window:\n",
    "    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(losses)), smoothed, linewidth=2, label='Smoothed Loss', color='red')\n",
    "else:\n",
    "    plt.plot(losses, linewidth=2, label='Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Smoothed, window={window})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textcaps_training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final average loss: {np.mean(losses[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: GENERATING TEST IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.load_lora_weights(OUTPUT_DIR)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded with LoRA weights\")\n",
    "\n",
    "# Test prompts - style matches TextCaps captions\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': \"A red sign that says 'STOP'\",\n",
    "        'expected': \"STOP\",\n",
    "        'filename': \"test_1_stop.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"A blue bottle with the word 'WATER' on it\",\n",
    "        'expected': \"WATER\",\n",
    "        'filename': \"test_2_water.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"A sign with 'EXIT' written on it\",\n",
    "        'expected': \"EXIT\",\n",
    "        'filename': \"test_3_exit.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"A poster that says 'SALE'\",\n",
    "        'expected': \"SALE\",\n",
    "        'filename': \"test_4_sale.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"A coffee mug with 'CAFE' on it\",\n",
    "        'expected': \"CAFE\",\n",
    "        'filename': \"test_5_cafe.png\"\n",
    "    },\n",
    "]\n",
    "\n",
    "negative_prompt = \"blurry, distorted, messy text, unclear letters, multiple objects, people, complex scene, watermark\"\n",
    "\n",
    "print(\"\\nGenerating test images...\")\n",
    "generated_images = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"  {i+1}/{len(test_cases)}: {test_case['prompt']}\")\n",
    "    \n",
    "    try:\n",
    "        image = pipe(\n",
    "            test_case['prompt'],\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512\n",
    "        ).images[0]\n",
    "        \n",
    "        image.save(test_case['filename'])\n",
    "        generated_images.append(test_case)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Generated {len(generated_images)} test images\")\n",
    "\n",
    "# Display generated images\n",
    "num_images = len(generated_images)\n",
    "cols = 3\n",
    "rows = (num_images + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, test_case in enumerate(generated_images):\n",
    "    img = Image.open(test_case['filename'])\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Expected: '{test_case['expected']}'\\n{test_case['prompt'][:40]}...\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(len(generated_images), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textcaps_generated_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Results visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: OCR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: QUANTITATIVE EVALUATION - OCR-based\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pytesseract\n",
    "import Levenshtein\n",
    "\n",
    "def preprocess_for_ocr(image_path):\n",
    "    \"\"\"Enhanced preprocessing for better OCR\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    blur = cv2.GaussianBlur(denoised, (5, 5), 0)\n",
    "    _, otsu = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return thresh, otsu\n",
    "\n",
    "def extract_text_multiple_strategies(image_path):\n",
    "    \"\"\"Try multiple OCR strategies\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    psm_modes = [\n",
    "        ('--psm 6', 'Block'),\n",
    "        ('--psm 8', 'Word'),\n",
    "        ('--psm 11', 'Sparse'),\n",
    "    ]\n",
    "    \n",
    "    for config, desc in psm_modes:\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(img, config=config).strip().upper()\n",
    "            results.append((text, desc))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    thresh, otsu = preprocess_for_ocr(image_path)\n",
    "    \n",
    "    if thresh is not None:\n",
    "        try:\n",
    "            text_thresh = pytesseract.image_to_string(thresh, config='--psm 8').strip().upper()\n",
    "            results.append((text_thresh, 'Adaptive'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if otsu is not None:\n",
    "        try:\n",
    "            text_otsu = pytesseract.image_to_string(otsu, config='--psm 8').strip().upper()\n",
    "            results.append((text_otsu, 'Otsu'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if not results:\n",
    "        return \"\", \"None\"\n",
    "    \n",
    "    best_result = max(results, key=lambda x: sum(c.isalnum() for c in x[0]))\n",
    "    return best_result[0], best_result[1]\n",
    "\n",
    "def evaluate_text_generation(image_path, expected_text):\n",
    "    \"\"\"Evaluate OCR accuracy\"\"\"\n",
    "    recovered_text, strategy = extract_text_multiple_strategies(image_path)\n",
    "    \n",
    "    clean_recovered = \"\".join(filter(str.isalnum, recovered_text))\n",
    "    clean_expected = \"\".join(filter(str.isalnum, expected_text.upper()))\n",
    "    \n",
    "    exact_match = 1.0 if clean_recovered == clean_expected else 0.0\n",
    "    \n",
    "    if len(clean_expected) == 0:\n",
    "        return 0.0, 0.0, recovered_text, strategy\n",
    "    \n",
    "    distance = Levenshtein.distance(clean_recovered, clean_expected)\n",
    "    char_accuracy = max(0, 1 - (distance / len(clean_expected)))\n",
    "    \n",
    "    return exact_match, char_accuracy, recovered_text, strategy\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nRunning OCR evaluation...\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Filename':<25} | {'Expected':<10} | {'OCR Result':<20} | {'Strategy':<15} | {'Exact':<6} | {'Char Acc'}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "results = []\n",
    "\n",
    "for test_case in generated_images:\n",
    "    image_path = test_case['filename']\n",
    "    expected = test_case['expected']\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        exact, acc, recovered, strategy = evaluate_text_generation(image_path, expected)\n",
    "        \n",
    "        results.append({\n",
    "            'filename': test_case['filename'],\n",
    "            'prompt': test_case['prompt'],\n",
    "            'expected': expected,\n",
    "            'recovered': recovered,\n",
    "            'exact': exact,\n",
    "            'accuracy': acc,\n",
    "            'strategy': strategy\n",
    "        })\n",
    "        \n",
    "        clean_recovered = \"\".join(filter(str.isalnum, recovered))[:20]\n",
    "        print(f\"{image_path:<25} | {expected:<10} | {clean_recovered:<20} | {strategy:<15} | {exact:<6.0f} | {acc:>7.1%}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "if results:\n",
    "    avg_exact = np.mean([r['exact'] for r in results])\n",
    "    avg_acc = np.mean([r['accuracy'] for r in results])\n",
    "    print(f\"{'AVERAGE':<25} | {'':<10} | {'':<20} | {'':<15} | {avg_exact:<6.2f} | {avg_acc:>7.1%}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results\n",
    "results_file = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {results_file}\")\n",
    "\n",
    "# Visualize\n",
    "if results:\n",
    "    fig, axes = plt.subplots(len(results), 3, figsize=(15, 5*len(results)))\n",
    "    if len(results) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        img = Image.open(result['filename'])\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(f\"Generated\\nExpected: '{result['expected']}'\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        thresh, otsu = preprocess_for_ocr(result['filename'])\n",
    "        \n",
    "        if thresh is not None:\n",
    "            axes[i, 1].imshow(thresh, cmap='gray')\n",
    "            axes[i, 1].set_title(\"Adaptive Threshold\", fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "        \n",
    "        if otsu is not None:\n",
    "            axes[i, 2].imshow(otsu, cmap='gray')\n",
    "            axes[i, 2].set_title(f\"Otsu\\nOCR: '{result['recovered'][:20]}'\", fontsize=10)\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('textcaps_ocr_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Final Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: FINAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare with base model\n",
    "print(\"\\nGenerating comparison with BASE model...\")\n",
    "\n",
    "base_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "base_pipe.to(\"cuda\")\n",
    "\n",
    "comparison_prompt = \"A red sign that says 'STOP'\"\n",
    "expected_text = \"STOP\"\n",
    "\n",
    "print(f\"  Prompt: {comparison_prompt}\")\n",
    "\n",
    "# Base model\n",
    "print(\"  Generating with BASE model...\")\n",
    "base_image = base_pipe(comparison_prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "base_image.save(\"comparison_base.png\")\n",
    "base_exact, base_acc, base_recovered, _ = evaluate_text_generation(\"comparison_base.png\", expected_text)\n",
    "\n",
    "# Fine-tuned model\n",
    "print(\"  Generating with FINE-TUNED model...\")\n",
    "finetuned_image = pipe(comparison_prompt, negative_prompt=negative_prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "finetuned_image.save(\"comparison_finetuned.png\")\n",
    "ft_exact, ft_acc, ft_recovered, _ = evaluate_text_generation(\"comparison_finetuned.png\", expected_text)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(base_image)\n",
    "axes[0].set_title(f\"Base SD 1.5\\nOCR: '{base_recovered[:20]}'\\nAccuracy: {base_acc:.1%}\", \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(finetuned_image)\n",
    "axes[1].set_title(f\"Fine-tuned (TextCaps)\\nOCR: '{ft_recovered[:20]}'\\nAccuracy: {ft_acc:.1%}\", \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Prompt: '{comparison_prompt}' | Expected: '{expected_text}'\", fontsize=13, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('textcaps_base_vs_finetuned.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Base Model:\")\n",
    "print(f\"  OCR: '{base_recovered}'\")\n",
    "print(f\"  Exact Match: {base_exact}\")\n",
    "print(f\"  Accuracy: {base_acc:.1%}\")\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  OCR: '{ft_recovered}'\")\n",
    "print(f\"  Exact Match: {ft_exact}\")\n",
    "print(f\"  Accuracy: {ft_acc:.1%}\")\n",
    "print(f\"\\nImprovement: {(ft_acc - base_acc)*100:+.1f} pp\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final Summary\n",
    "summary = f\"\"\"\n",
    "TEXTCAPS FINE-TUNING SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Dataset: TextCaps (lmms-lab/TextCaps)\n",
    "Training Samples: {len(preprocessed_data)}\n",
    "Training Steps: {TRAIN_STEPS}\n",
    "Final Loss: {losses[-1]:.4f}\n",
    "Avg Loss (last 100): {np.mean(losses[-100:]):.4f}\n",
    "\n",
    "Evaluation:\n",
    "- Test Cases: {len(results)}\n",
    "- Avg Exact Match: {avg_exact:.2f}\n",
    "- Avg Character Accuracy: {avg_acc:.1%}\n",
    "\n",
    "Observations:\n",
    "1. TextCaps = COCO images with text descriptions\n",
    "2. Captions describe scenes, not just text\n",
    "3. Model learns scene generation + text rendering\n",
    "4. Text is contextual (signs, products, labels)\n",
    "\n",
    "Challenges:\n",
    "- Complex scenes with text embedded\n",
    "- Captions describe full context\n",
    "- Limited clean text-only samples\n",
    "- Text quality varies significantly\n",
    "\n",
    "Recommendations:\n",
    "- Increase training steps (2000+)\n",
    "- Better filtering for simple text images\n",
    "- Try synthetic data for cleaner learning\n",
    "- Consider text-specific pre-trained models\n",
    "\n",
    "Files Saved:\n",
    "- Model: {OUTPUT_DIR}/\n",
    "- Results: {OUTPUT_DIR}/evaluation_results.json\n",
    "- Plots: textcaps_*.png\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/final_summary.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n✓ Summary saved\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXTCAPS TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
