{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Stable Diffusion for Text Generation using TextOCR Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates fine-tuning Stable Diffusion v1.5 with LoRA adapters to improve text rendering capabilities using the TextOCR dataset.\n",
    "\n",
    "**Flow:**\n",
    "1. Image + text as input → Model\n",
    "2. Model trained with LoRA\n",
    "3. Give a prompt → Generate image\n",
    "4. Evaluate with OCR (Exact Match + Character Accuracy)\n",
    "\n",
    "**Dataset:** TextOCR (facebook/textocr)\n",
    "**Model:** Stable Diffusion v1.5 with LoRA fine-tuning\n",
    "**Evaluation:** OCR-based readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SETUP: Installing Required Packages\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!pip install -q diffusers transformers accelerate peft datasets pillow pytesseract python-Levenshtein opencv-python-headless\n",
    "\n",
    "# Install Tesseract OCR for Colab\n",
    "!apt-get install -y tesseract-ocr\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"./text-render-lora-textocr\"\n",
    "MAX_LENGTH = 77\n",
    "IMAGE_SIZE = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_STEPS = 800  # Increased for better convergence\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"  Device: {accelerator.device}\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Exploratory Data Analysis - TextOCR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: EXPLORATORY DATA ANALYSIS - TextOCR Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load TextOCR dataset\n",
    "print(\"\\nLoading TextOCR dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"facebook/textocr\", split=\"train\", streaming=True)\n",
    "    print(\"✓ TextOCR dataset loaded (streaming mode)\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Attempting alternative loading method...\")\n",
    "    dataset = load_dataset(\"facebook/textocr\", split=\"validation\", streaming=True)\n",
    "\n",
    "# Sample initial examples for EDA\n",
    "print(\"\\nSampling examples for analysis...\")\n",
    "samples = []\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= 30:\n",
    "        break\n",
    "    samples.append(example)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {i + 1} samples...\")\n",
    "\n",
    "print(f\"\\n✓ Sampled {len(samples)} examples\")\n",
    "\n",
    "# Analyze dataset structure\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Dataset Structure:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Available keys: {samples[0].keys()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Examples:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(min(5, len(samples))):\n",
    "    sample = samples[i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Keys: {sample.keys()}\")\n",
    "    \n",
    "    if 'image' in sample:\n",
    "        img = sample['image']\n",
    "        print(f\"  Image size: {img.size if hasattr(img, 'size') else 'N/A'}\")\n",
    "    \n",
    "    if 'text' in sample:\n",
    "        texts = sample['text']\n",
    "        if isinstance(texts, list):\n",
    "            print(f\"  Texts found: {len(texts)} annotations\")\n",
    "            print(f\"  Sample texts: {texts[:3]}\")\n",
    "        else:\n",
    "            print(f\"  Text: {str(texts)[:100]}\")\n",
    "    \n",
    "    if 'caption' in sample:\n",
    "        print(f\"  Caption: {str(sample['caption'])[:100]}\")\n",
    "\n",
    "# Visualize samples\n",
    "print(\"\\nVisualizing sample images...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, len(samples))):\n",
    "    sample = samples[i]\n",
    "    if 'image' in sample:\n",
    "        img = sample['image']\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        title = \"TextOCR Sample\"\n",
    "        if 'text' in sample:\n",
    "            texts = sample['text']\n",
    "            if isinstance(texts, list) and len(texts) > 0:\n",
    "                title = f\"Texts: {', '.join(str(t) for t in texts[:2])}\"\n",
    "            else:\n",
    "                title = f\"Text: {str(texts)[:30]}\"\n",
    "        \n",
    "        axes[i].set_title(title, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textocr_samples_eda.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ EDA complete - samples saved to 'textocr_samples_eda.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Filtering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: DATA FILTERING AND PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\")\n",
    "\n",
    "def extract_text_from_sample(sample):\n",
    "    \"\"\"Extract text annotations from TextOCR sample\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    if 'text' in sample:\n",
    "        text_data = sample['text']\n",
    "        if isinstance(text_data, list):\n",
    "            texts.extend([str(t) for t in text_data if t])\n",
    "        elif text_data:\n",
    "            texts.append(str(text_data))\n",
    "    \n",
    "    if 'caption' in sample:\n",
    "        caption = sample['caption']\n",
    "        if caption:\n",
    "            texts.append(str(caption))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def is_good_textocr_sample(sample):\n",
    "    \"\"\"Filter for samples with clear, readable text\"\"\"\n",
    "    texts = extract_text_from_sample(sample)\n",
    "    \n",
    "    if not texts:\n",
    "        return False\n",
    "    \n",
    "    num_texts = len(texts)\n",
    "    \n",
    "    if num_texts == 0 or num_texts > 5:\n",
    "        return False\n",
    "    \n",
    "    valid_texts = [t for t in texts if 2 <= len(t) <= 30 and t.strip()]\n",
    "    \n",
    "    if len(valid_texts) == 0:\n",
    "        return False\n",
    "    \n",
    "    alphanumeric_texts = [t for t in valid_texts if any(c.isalpha() for c in t)]\n",
    "    \n",
    "    return len(alphanumeric_texts) > 0\n",
    "\n",
    "def create_caption_from_textocr(sample):\n",
    "    \"\"\"Create a descriptive caption for training\"\"\"\n",
    "    texts = extract_text_from_sample(sample)\n",
    "    \n",
    "    if not texts:\n",
    "        return \"An image with text\"\n",
    "    \n",
    "    main_text = max(texts, key=len)\n",
    "    \n",
    "    captions = [\n",
    "        f\"An image with the text '{main_text}'\",\n",
    "        f\"A photo showing '{main_text}' written on it\",\n",
    "        f\"Text that says '{main_text}'\",\n",
    "        f\"An image containing the word '{main_text}'\",\n",
    "    ]\n",
    "    \n",
    "    return np.random.choice(captions)\n",
    "\n",
    "def preprocess_textocr_sample(sample):\n",
    "    \"\"\"Convert TextOCR sample to tensors\"\"\"\n",
    "    image = sample['image'].convert('RGB').resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "    image_array = np.array(image).astype(np.float32) / 127.5 - 1.0\n",
    "    pixel_values = torch.from_numpy(image_array).permute(2, 0, 1)\n",
    "    \n",
    "    caption = create_caption_from_textocr(sample)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        caption,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs.input_ids[0]\n",
    "    \n",
    "    texts = extract_text_from_sample(sample)\n",
    "    ground_truth = texts[0] if texts else \"\"\n",
    "    \n",
    "    return pixel_values, input_ids, caption, ground_truth\n",
    "\n",
    "# Filter and collect good samples\n",
    "print(\"\\nFiltering TextOCR dataset for text-focused samples...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "dataset_stream = load_dataset(\"facebook/textocr\", split=\"train\", streaming=True)\n",
    "\n",
    "filtered_samples = []\n",
    "processed_count = 0\n",
    "\n",
    "for example in dataset_stream:\n",
    "    processed_count += 1\n",
    "    \n",
    "    if len(filtered_samples) >= 300:\n",
    "        break\n",
    "    \n",
    "    if processed_count >= 3000:\n",
    "        print(f\"  Searched {processed_count} samples, found {len(filtered_samples)}\")\n",
    "        break\n",
    "    \n",
    "    if is_good_textocr_sample(example):\n",
    "        filtered_samples.append(example)\n",
    "        \n",
    "        if len(filtered_samples) % 50 == 0:\n",
    "            print(f\"  Found {len(filtered_samples)} suitable samples (searched {processed_count})...\")\n",
    "\n",
    "print(f\"\\n✓ Filtered dataset: {len(filtered_samples)} text-focused samples from {processed_count} total\")\n",
    "\n",
    "# Show filtered examples\n",
    "print(\"\\nFiltered Sample Captions:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(min(5, len(filtered_samples))):\n",
    "    caption = create_caption_from_textocr(filtered_samples[i])\n",
    "    texts = extract_text_from_sample(filtered_samples[i])\n",
    "    print(f\"{i+1}. Caption: {caption}\")\n",
    "    print(f\"   Ground truth texts: {texts}\")\n",
    "\n",
    "# Visualize filtered samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, len(filtered_samples))):\n",
    "    sample = filtered_samples[i]\n",
    "    img = sample['image']\n",
    "    caption = create_caption_from_textocr(sample)\n",
    "    texts = extract_text_from_sample(sample)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{caption}\\nTexts: {', '.join(texts[:2])}\", fontsize=7)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textocr_filtered_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Filtered samples visualization saved\")\n",
    "\n",
    "# Preprocess all samples\n",
    "print(\"\\nPreprocessing training data...\")\n",
    "preprocessed_data = []\n",
    "\n",
    "for sample in tqdm(filtered_samples, desc=\"Preprocessing\"):\n",
    "    try:\n",
    "        pixel_values, input_ids, caption, ground_truth = preprocess_textocr_sample(sample)\n",
    "        preprocessed_data.append({\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': input_ids,\n",
    "            'caption': caption,\n",
    "            'ground_truth': ground_truth\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ {len(preprocessed_data)} samples preprocessed and ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: MODEL INITIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Core Components\n",
    "print(\"Loading Stable Diffusion v1.5 components...\")\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\")\n",
    "\n",
    "print(\"✓ Base models loaded\")\n",
    "\n",
    "# Freeze original weights (only train LoRA adapters)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"✓ Base model weights frozen\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to UNet\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "\n",
    "print(\"✓ LoRA adapters added to UNet\")\n",
    "\n",
    "# Move to device\n",
    "device = accelerator.device\n",
    "unet.to(device)\n",
    "vae.to(device, dtype=torch.float16)\n",
    "text_encoder.to(device, dtype=torch.float16)\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in unet.parameters())\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model Statistics:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"  Trainable percentage: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  VAE scaling factor: {vae.config.scaling_factor}\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STEP 4: TRAINING - {TRAIN_STEPS} steps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "unet.train()\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Training steps: {TRAIN_STEPS}\")\n",
    "print(f\"  Training samples: {len(preprocessed_data)}\")\n",
    "print(f\"  Epochs (approx): {TRAIN_STEPS / len(preprocessed_data):.1f}\")\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "unet_dtype = next(unet.parameters()).dtype\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "progress_bar = tqdm(range(TRAIN_STEPS), desc=\"Training\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    # Get data (cycle through dataset)\n",
    "    idx = step % len(preprocessed_data)\n",
    "    data_item = preprocessed_data[idx]\n",
    "    \n",
    "    pixel_values = data_item['pixel_values']\n",
    "    input_ids = data_item['input_ids']\n",
    "    \n",
    "    # Move to GPU and add batch dimension\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device, dtype=torch.float16)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    # VAE ENCODING - Convert image to latent space\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        # CRITICAL: Scale latents by VAE scaling factor\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "    \n",
    "    # Create random noise for diffusion training\n",
    "    noise = torch.randn_like(latents)\n",
    "    \n",
    "    # Sample random timestep\n",
    "    timesteps = torch.randint(\n",
    "        0, \n",
    "        noise_scheduler.config.num_train_timesteps, \n",
    "        (1,), \n",
    "        device=device\n",
    "    ).long()\n",
    "    \n",
    "    # Add noise to latents (forward diffusion process)\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    \n",
    "    # Get text embeddings\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "    \n",
    "    # Ensure dtype consistency for LoRA\n",
    "    noisy_latents = noisy_latents.to(dtype=unet_dtype)\n",
    "    encoder_hidden_states = encoder_hidden_states.to(dtype=unet_dtype)\n",
    "    \n",
    "    # Predict noise using UNet\n",
    "    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        model_pred.float(), \n",
    "        noise.float(), \n",
    "        reduction=\"mean\"\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Record loss\n",
    "    loss_value = loss.item()\n",
    "    losses.append(loss_value)\n",
    "    \n",
    "    # Update progress bar\n",
    "    if step % 10 == 0:\n",
    "        avg_loss = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
    "        progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    # Detailed logging every 100 steps\n",
    "    if (step + 1) % 100 == 0:\n",
    "        avg_loss = np.mean(losses[-100:])\n",
    "        print(f\"\\n  Step {step+1}/{TRAIN_STEPS}: Avg Loss (last 100) = {avg_loss:.4f}\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "print(\"\\nSaving model...\")\n",
    "unet.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"✓ Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    'model': MODEL_NAME,\n",
    "    'training_steps': TRAIN_STEPS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_samples': len(preprocessed_data),\n",
    "    'final_loss': losses[-1] if losses else None,\n",
    "    'avg_loss_last_100': np.mean(losses[-100:]) if len(losses) >= 100 else np.mean(losses)\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✓ Training metadata saved\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Raw loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, alpha=0.3, label='Raw Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Raw)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Smoothed loss\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 50\n",
    "if len(losses) >= window:\n",
    "    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(losses)), smoothed, linewidth=2, label='Smoothed Loss', color='red')\n",
    "else:\n",
    "    plt.plot(losses, linewidth=2, label='Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Smoothed, window={window})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textocr_training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final average loss: {np.mean(losses[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: GENERATING TEST IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.load_lora_weights(OUTPUT_DIR)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded with LoRA weights\")\n",
    "\n",
    "# Test prompts based on common text patterns\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': \"An image with the text 'STOP' written on it\",\n",
    "        'expected': \"STOP\",\n",
    "        'filename': \"test_1_stop.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"A photo showing 'OPEN' written on it\",\n",
    "        'expected': \"OPEN\",\n",
    "        'filename': \"test_2_open.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"Text that says 'EXIT'\",\n",
    "        'expected': \"EXIT\",\n",
    "        'filename': \"test_3_exit.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"An image containing the word 'SALE'\",\n",
    "        'expected': \"SALE\",\n",
    "        'filename': \"test_4_sale.png\"\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"An image with the text 'CAFE' written on it\",\n",
    "        'expected': \"CAFE\",\n",
    "        'filename': \"test_5_cafe.png\"\n",
    "    },\n",
    "]\n",
    "\n",
    "negative_prompt = \"blurry, distorted, messy text, unclear letters, multiple objects, people, complex scene, watermark\"\n",
    "\n",
    "print(\"\\nGenerating test images...\")\n",
    "generated_images = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"  {i+1}/{len(test_cases)}: {test_case['prompt']}\")\n",
    "    \n",
    "    try:\n",
    "        image = pipe(\n",
    "            test_case['prompt'],\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512\n",
    "        ).images[0]\n",
    "        \n",
    "        image.save(test_case['filename'])\n",
    "        generated_images.append(test_case)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error generating image: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Generated {len(generated_images)} test images\")\n",
    "\n",
    "# Display generated images\n",
    "num_images = len(generated_images)\n",
    "cols = 3\n",
    "rows = (num_images + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, test_case in enumerate(generated_images):\n",
    "    img = Image.open(test_case['filename'])\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Expected: '{test_case['expected']}'\\n{test_case['prompt'][:40]}...\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide extra subplots\n",
    "for i in range(len(generated_images), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('textocr_generated_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Results visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: OCR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: QUANTITATIVE EVALUATION - OCR-based\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pytesseract\n",
    "import Levenshtein\n",
    "\n",
    "def preprocess_for_ocr(image_path):\n",
    "    \"\"\"Enhanced preprocessing for better OCR\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    blur = cv2.GaussianBlur(denoised, (5, 5), 0)\n",
    "    _, otsu = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return thresh, otsu\n",
    "\n",
    "def extract_text_multiple_strategies(image_path):\n",
    "    \"\"\"Try multiple OCR strategies and return best result\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    psm_modes = [\n",
    "        ('--psm 6', 'Block of text'),\n",
    "        ('--psm 8', 'Single word'),\n",
    "        ('--psm 11', 'Sparse text'),\n",
    "    ]\n",
    "    \n",
    "    for config, desc in psm_modes:\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(img, config=config).strip().upper()\n",
    "            results.append((text, desc))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    thresh, otsu = preprocess_for_ocr(image_path)\n",
    "    \n",
    "    if thresh is not None:\n",
    "        try:\n",
    "            text_thresh = pytesseract.image_to_string(thresh, config='--psm 8').strip().upper()\n",
    "            results.append((text_thresh, 'Adaptive threshold'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if otsu is not None:\n",
    "        try:\n",
    "            text_otsu = pytesseract.image_to_string(otsu, config='--psm 8').strip().upper()\n",
    "            results.append((text_otsu, 'Otsu threshold'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if not results:\n",
    "        return \"\", \"No strategy worked\"\n",
    "    \n",
    "    best_result = max(results, key=lambda x: sum(c.isalnum() for c in x[0]))\n",
    "    return best_result[0], best_result[1]\n",
    "\n",
    "def evaluate_text_generation(image_path, expected_text):\n",
    "    \"\"\"Evaluate OCR accuracy against expected text\"\"\"\n",
    "    recovered_text, strategy = extract_text_multiple_strategies(image_path)\n",
    "    \n",
    "    clean_recovered = \"\".join(filter(str.isalnum, recovered_text))\n",
    "    clean_expected = \"\".join(filter(str.isalnum, expected_text.upper()))\n",
    "    \n",
    "    exact_match = 1.0 if clean_recovered == clean_expected else 0.0\n",
    "    \n",
    "    if len(clean_expected) == 0:\n",
    "        return 0.0, 0.0, recovered_text, strategy\n",
    "    \n",
    "    distance = Levenshtein.distance(clean_recovered, clean_expected)\n",
    "    char_accuracy = max(0, 1 - (distance / len(clean_expected)))\n",
    "    \n",
    "    return exact_match, char_accuracy, recovered_text, strategy\n",
    "\n",
    "# Evaluate all generated images\n",
    "print(\"\\nRunning OCR evaluation on generated images...\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Filename':<25} | {'Expected':<10} | {'OCR Result':<20} | {'Strategy':<20} | {'Exact':<6} | {'Char Acc'}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "results = []\n",
    "\n",
    "for test_case in generated_images:\n",
    "    image_path = test_case['filename']\n",
    "    expected = test_case['expected']\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        exact, acc, recovered, strategy = evaluate_text_generation(image_path, expected)\n",
    "        \n",
    "        results.append({\n",
    "            'filename': test_case['filename'],\n",
    "            'prompt': test_case['prompt'],\n",
    "            'expected': expected,\n",
    "            'recovered': recovered,\n",
    "            'exact': exact,\n",
    "            'accuracy': acc,\n",
    "            'strategy': strategy\n",
    "        })\n",
    "        \n",
    "        clean_recovered = \"\".join(filter(str.isalnum, recovered))[:20]\n",
    "        \n",
    "        print(f\"{image_path:<25} | {expected:<10} | {clean_recovered:<20} | {strategy:<20} | {exact:<6.0f} | {acc:>7.1%}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"-\"*100)\n",
    "if results:\n",
    "    avg_exact = np.mean([r['exact'] for r in results])\n",
    "    avg_acc = np.mean([r['accuracy'] for r in results])\n",
    "    print(f\"{'AVERAGE':<25} | {'':<10} | {'':<20} | {'':<20} | {avg_exact:<6.2f} | {avg_acc:>7.1%}\")\n",
    "else:\n",
    "    print(\"No results to evaluate\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save detailed results\n",
    "results_file = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Detailed results saved to {results_file}\")\n",
    "\n",
    "# Visualize OCR preprocessing steps\n",
    "if results:\n",
    "    fig, axes = plt.subplots(len(results), 3, figsize=(15, 5*len(results)))\n",
    "    if len(results) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        image_path = result['filename']\n",
    "        \n",
    "        img = Image.open(image_path)\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(f\"Generated Image\\nExpected: '{result['expected']}'\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        thresh, otsu = preprocess_for_ocr(image_path)\n",
    "        \n",
    "        if thresh is not None:\n",
    "            axes[i, 1].imshow(thresh, cmap='gray')\n",
    "            axes[i, 1].set_title(\"Adaptive Threshold\", fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "        \n",
    "        if otsu is not None:\n",
    "            axes[i, 2].imshow(otsu, cmap='gray')\n",
    "            axes[i, 2].set_title(f\"Otsu Threshold\\nOCR: '{result['recovered'][:20]}'\", fontsize=10)\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('textocr_ocr_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ OCR preprocessing visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Final Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: FINAL ANALYSIS AND COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate comparison with base model (no fine-tuning)\n",
    "print(\"\\nGenerating comparison with BASE model (no fine-tuning)...\")\n",
    "\n",
    "base_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "base_pipe.to(\"cuda\")\n",
    "\n",
    "comparison_prompt = \"An image with the text 'STOP' written on it\"\n",
    "expected_text = \"STOP\"\n",
    "\n",
    "print(f\"  Prompt: {comparison_prompt}\")\n",
    "print(f\"  Expected text: {expected_text}\")\n",
    "\n",
    "# Generate with base model\n",
    "print(\"\\n  Generating with BASE model...\")\n",
    "base_image = base_pipe(\n",
    "    comparison_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "base_image.save(\"comparison_base.png\")\n",
    "\n",
    "# Evaluate base model\n",
    "base_exact, base_acc, base_recovered, _ = evaluate_text_generation(\"comparison_base.png\", expected_text)\n",
    "\n",
    "# Generate with fine-tuned model\n",
    "print(\"  Generating with FINE-TUNED model...\")\n",
    "finetuned_image = pipe(\n",
    "    comparison_prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "finetuned_image.save(\"comparison_finetuned.png\")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "ft_exact, ft_acc, ft_recovered, _ = evaluate_text_generation(\"comparison_finetuned.png\", expected_text)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(base_image)\n",
    "axes[0].set_title(f\"Base SD 1.5 (No Fine-tuning)\\nOCR: '{base_recovered[:20]}'\\nAccuracy: {base_acc:.1%}\", \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(finetuned_image)\n",
    "axes[1].set_title(f\"After LoRA Fine-tuning (TextOCR)\\nOCR: '{ft_recovered[:20]}'\\nAccuracy: {ft_acc:.1%}\", \n",
    "                  fontsize=11, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Prompt: '{comparison_prompt}' | Expected: '{expected_text}'\", fontsize=13, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('textocr_base_vs_finetuned.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Base Model:\")\n",
    "print(f\"  OCR Result: '{base_recovered}'\")\n",
    "print(f\"  Exact Match: {base_exact}\")\n",
    "print(f\"  Character Accuracy: {base_acc:.1%}\")\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  OCR Result: '{ft_recovered}'\")\n",
    "print(f\"  Exact Match: {ft_exact}\")\n",
    "print(f\"  Character Accuracy: {ft_acc:.1%}\")\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Accuracy Delta: {(ft_acc - base_acc)*100:+.1f} percentage points\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - TextOCR Fine-tuning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Dataset: TextOCR (facebook/textocr)\n",
    "Training Samples: {len(preprocessed_data)}\n",
    "Training Steps: {TRAIN_STEPS}\n",
    "Final Loss: {losses[-1]:.4f}\n",
    "Average Loss (last 100 steps): {np.mean(losses[-100:]):.4f}\n",
    "\n",
    "Evaluation Results:\n",
    "- Test Cases: {len(results)}\n",
    "- Average Exact Match: {avg_exact:.2f}\n",
    "- Average Character Accuracy: {avg_acc:.1%}\n",
    "\n",
    "Key Observations:\n",
    "1. TextOCR contains real-world images with text annotations\n",
    "2. The model learned from crowd-sourced text-image pairs\n",
    "3. Fine-tuning adapted the model to generate images with text\n",
    "4. OCR evaluation provides quantitative metrics\n",
    "\n",
    "Challenges:\n",
    "- TextOCR images are complex real-world scenes\n",
    "- Text is often embedded in context (signs, products, etc.)\n",
    "- Model learns both scene generation AND text rendering\n",
    "- Limited training steps due to computational constraints\n",
    "\n",
    "Next Steps:\n",
    "- Increase training steps (2000+) for better convergence\n",
    "- Use more filtered samples (focus on simple text)\n",
    "- Experiment with different LoRA configurations\n",
    "- Try higher resolution (768x768) if GPU allows\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/final_summary.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n✓ Summary saved to {OUTPUT_DIR}/final_summary.txt\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXTOCR TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
